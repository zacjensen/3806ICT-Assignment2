\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage{url}
\usepackage{hyperref}
\usepackage{subcaption}

\geometry{left = 2cm, right=2cm, bottom=2cm, top=2cm}

\title{3806ICT - Maze Planning and Replanning with Reinforcement Learning}
\author{Zac Jensen - s5153515 - zac.jensen@griffithuni.edu.au \\
    Jake Muggleton - s5130398 - jake.muggleton@griffithuni.edu.au}

\pagestyle{fancy}
\renewcommand{\headrulewidth}{1pt}
\fancyhf{}
\rhead{3806ICT - Maze Solving}
\chead{Griffith University}
\lhead{}
\rfoot{Page \thepage}
\newcommand\tab[1][1cm]{\hspace*{#1}}

\begin{document}
    \maketitle

%==============================================================================

    \section{Abstract}\label{sec:abstract}
        This report explores the problem of autonomous agent planning and replanning in a dynamic environment. In this report, an agent uses Q-learning (reinforcement learning) to navigate through a 2D grid-maze environment towards a goal position. During its navigation however, obstacles in the environment, and the goal position may change. The agent uses replanning techniques to navigate this dynamic environment to successfully reach the goal. A model for such a scenario is presented which is then tested for mazes of various sizes. In most cases, the agent is able to successfully navigate the dynamic environment towards the goal. 

    \section{Introduction}\label{sec:introduction}
        A maze world is defined as a 2D matrix of ASCII characters where ‘O’ is an open cell that the agent can step to, ‘H’ is a closed wall that the agent cannot step to, ‘G’ is the goal position for the agent, and ‘S’ is the start position for the agent. The agent may move around the maze by taking ‘steps’, where each step may involve moving up,down,left, or right by one cell. In this report, first a model representing an agent navigating a static (unchanging) maze world is shown. Next, a method for creating a dynamic environment is proposed. Finally, experiments on this model are conducted and an analysis of the results is performed.

    \section{Task 2 - Planning in a Static Environment}\label{sec:test-design}
        To model an agent navigating a static environment, a maze generation program aswell as a Q-learning based Reinforcement Learning (RL) algorithm were used which are available from the supplied ‘2021-assignment-sample2’ folder [1]. This model is available as ‘GridWorld-task2.py’ and can be run using the command line ‘python3 GridWorld-task2.py [maze\_width] [maze\_height]’.

In this model, a maze is randomly generated with a start point and goal point using a common random depth-first search (DFS) based algorithm. The Q-learning algorithm then provides a set of steps (a trace) towards the goal from the start. An extra constraint for this model is that the agent can take no more than 2N steps where N is the width of the NxN maze. 

The model is outlined in the following pseudocode:\\
\hspace*{4cm}maze $\leftarrow$ generateMaze(height, width)\\
\hspace*{4cm}RLSolver $\leftarrow$ initialise Q-Learning solver\\
\hspace*{4cm}trace $\leftarrow$ RLSolver.solve(maze)\\
\hspace*{4cm}if trace.length $<$ 2*width:\\
\hspace*{5cm}goal $\leftarrow$ reached


\section{Task 3 - Programming A Dynamic Environment}\label{sec:test-design}
	To model a dynamic environment, a function was created which takes an existing maze and returns a new maze which is a ‘mutation’ of the original. This model is available as ‘GridWorld-task3.py’ which calls mutate\_maze() in ‘MutateMaze.py’ and can be run using the command line ‘python3 GridWorld-task3.py [maze\_width] [maze\_height]’. 

	The constraints for this are that the new maze must share similarities to the old one, and obstacles in the new maze mustn’t overlap with the current, or previous positions of the agent. The goal must also not be blocked off from the agent by obstacles. To generate a mutation, first all of the previous positions of the agent are collected. Next, multiple random walks are conducted from the agents current position in the original maze, keeping track of all cells visited by the search. A new maze is initialised with all of the agent’s previous positions and cells visited by the random walks and the DFS maze generation algorithm is run to generate a new maze containing these cells. Finally, the new goal location is set using a random walk, where the walk becomes shorter if the agent has taken alot of steps. This mutation method generates a new maze that guarantees that the goal is reachable and the agent or its previous positions are not overlapped by an obstacle. It also produces a maze that has a similar structure to the old maze for all cells close to the agent, but becomes increasingly different for cells further away from the agent. 

	The model is demonstrated by using the model from task 2, and then mutating the maze once the agent reaches the goal. Figure 1 shows the three steps involved in a maze mutation including the original maze, initialising the new maze with the previous positions of the agent aswell as the paths of multiple random walks, and finally the newly generated maze. Here, ‘u’ is a maze element which hasn’t yet been initialised. 

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{assets/mutateMaze.png}
	\caption{The Three Steps of A Maze Mutation. Initial Maze (Left), Choosing Maze Features To Keep (Middle), Mutated Maze (Right)}
\end{figure}


\section{Task 4 - Replanning in a Dynamic Environment}\label{sec:test-design}

	To model planning and replanning in a dynamic environment, the agent in task 2 and the mutation function in task 3 were combined. This model is available as ‘GridWorld-task4.py’ and can be run using the command line ‘python3 GridWorld-task4.py [maze\_width] [maze\_height]’.

Here, after calling the program on the command line, an initial maze is generated. The agent then autonomously plans a route towards the goal and starts taking steps towards it. At each step there is a 10\% chance that the maze mutates. If so, the maze is mutated with the function created in task 3. The agent then replans its route by solving the maze from its current position. Finally, the program terminates when either the agent reaches the goal, or the agent has taken more than 2*width steps, in which case the agent is reported to have failed. \\

The model is outlined in the following pseudocode:\\
\hspace*{2cm}maze $\leftarrow$ generateMaze(height, width)\\
\hspace*{2cm}agent $\leftarrow$ agent object positioned at maze.start\\
\hspace*{2cm}steps $\leftarrow$ empty list\\
\hspace*{2cm}while agent has not reached goal and steps.size $<$ 2*width\\
\hspace*{3cm}trace $\leftarrow$ RLSolver.solve(maze)\\
\hspace*{3cm}for step in trace:\\
	\hspace*{4cm}steps.append(step)\\
	\hspace*{4cm}agent $\leftarrow$ update agent position to step\\
	\hspace*{4cm}if 10\% chance occurs: mutate maze and break from for loop\\
\hspace*{2cm}check if agent at goal


    \section{Results}\label{sec:results}

        \begin{tabular}{ | m{1cm} | m{5em}| m{2cm} | m{2cm} | m{2cm} | m{2cm} | m{2cm} | m{2cm} | } 
	\hline
	Size & Number of Times Goal was reached & Minimum Path Length & Maximum Path Length & Average Path Length & Average time taken
	per maze (s) & Average number of Mutations & Total time taken (s)\\
	  \hline
	  10 & 10 & 2 & 18 & 10 & 1.26 & 0.6 & 12.61 \\ 
	  \hline
	  50 & 10 & 19 & 59 & 38.5 & 46.49 & 3.7 & 464.87 \\ 
	  \hline
	  100 & 10 & 10 & 99 & 53.8 & 99.6 & 5.4 & 996.7 \\ 
	  \hline
	\end{tabular}
	\vspace*{1cm} \\
	As we can see from the table above

     

%==============================================================================
    \bibliographystyle{apalike}
    \bibliography{biblio}
\end{document}
